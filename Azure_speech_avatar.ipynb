{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMvIu3TnHrXmRd15qOruhJg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Craigy101/Azure/blob/main/Azure_speech_avatar.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RTINY4Mz7Nq1",
        "outputId": "bff69405-fec9-4f2e-884b-9365ad287946"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'cognitive-services-speech-sdk'...\n",
            "remote: Enumerating objects: 61485, done.\u001b[K\n",
            "remote: Counting objects: 100% (4552/4552), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1715/1715), done.\u001b[K\n",
            "remote: Total 61485 (delta 2705), reused 4137 (delta 2426), pack-reused 56933\u001b[K\n",
            "Receiving objects: 100% (61485/61485), 263.40 MiB | 25.79 MiB/s, done.\n",
            "Resolving deltas: 100% (32158/32158), done.\n",
            "Updating files: 100% (2193/2193), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Azure-Samples/cognitive-services-speech-sdk.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install azure-cognitiveservices-speech"
      ],
      "metadata": {
        "id": "saxhgX5j8vyB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd cognitive-services-speech-sdk/samples/python/web/avatar"
      ],
      "metadata": {
        "id": "fgfoRE0_7s3k"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "9O8vXSgj8v8C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%env SPEECH_KEY=your-key\n",
        "%env SPEECH_REGION=your-region\n"
      ],
      "metadata": {
        "id": "4gr4Xl-H73m2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import azure.cognitiveservices.speech as speechsdk\n",
        "\n",
        "# This example requires environment variables named \"SPEECH_KEY\" and \"SPEECH_REGION\"\n",
        "speech_config = speechsdk.SpeechConfig(subscription=os.environ.get('SPEECH_KEY'), region=os.environ.get('SPEECH_REGION'))\n",
        "audio_config = speechsdk.audio.AudioOutputConfig(use_default_speaker=True)\n",
        "\n",
        "# The neural multilingual voice can speak different languages based on the input text.\n",
        "speech_config.speech_synthesis_voice_name='en-US-AvaMultilingualNeural'\n",
        "\n",
        "speech_synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config, audio_config=audio_config)\n",
        "\n",
        "# Get text from the console and synthesize to the default speaker.\n",
        "print(\"Enter some text that you want to speak >\")\n",
        "text = input()\n",
        "\n",
        "speech_synthesis_result = speech_synthesizer.speak_text_async(text).get()\n",
        "\n",
        "if speech_synthesis_result.reason == speechsdk.ResultReason.SynthesizingAudioCompleted:\n",
        "    print(\"Speech synthesized for text [{}]\".format(text))\n",
        "elif speech_synthesis_result.reason == speechsdk.ResultReason.Canceled:\n",
        "    cancellation_details = speech_synthesis_result.cancellation_details\n",
        "    print(\"Speech synthesis canceled: {}\".format(cancellation_details.reason))\n",
        "    if cancellation_details.reason == speechsdk.CancellationReason.Error:\n",
        "        if cancellation_details.error_details:\n",
        "            print(\"Error details: {}\".format(cancellation_details.error_details))\n",
        "            print(\"Did you set the speech resource key and region values?\")"
      ],
      "metadata": {
        "id": "6unbZstt8tgc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m flask run -h 0.0.0.0 -p 5000\n",
        "#http://localhost:5000 to view the output"
      ],
      "metadata": {
        "id": "nv-BFB3n9wcA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Fill or select below information:\n",
        "\n",
        "TTS Configuration\n",
        "TTS Voice - the voice of the TTS. Here is the available TTS voices list\n",
        "Custom Voice Deployment ID (Endpoint ID) - the deployment ID (also called endpoint ID) of your custom voice. If you are not using a custom voice, please leave it empty.\n",
        "Personal Voice Speaker Profile ID - the personal voice speaker profile ID of your personal voice. Please follow here to view and create personal voice.\n",
        "Enable Private Endpoint - check this if you want to apply private endpoint. If you check this, you need to fill Private Endpoint section below. Please refer to speech-services-private-link to learn more about private endpoint.\n",
        "Private EndPoint - the private endpoint of your Azure speech resource. The format should be like: https://{your custom name}.cognitiveservices.azure.com.\n",
        "Avatar Configuration\n",
        "Avatar Character - The character of the avatar. By default it's lisa, and you can update this value to use a different avatar.\n",
        "Avatar Style - The style of the avatar. You can update this value to use a different avatar style. This parameter is optional for custom avatar.\n",
        "Background Color - The color of the avatar background.\n",
        "Custom Avatar - Check this if you are using a custom avatar.\n",
        "Transparent Background - Check this if you want to use transparent background for the avatar. When this is checked, the background color of the video stream from server side is automatically set to green(#00FF00FF), and the js code on client side (check the makeBackgroundTransparent function in main.js) will do the real-time matting by replacing the green color with transparent color.\n",
        "Video Crop - By checking this, you can crop the video stream from server side to a smaller size. This is useful when you want to put the avatar video into a customized rectangle area.\n",
        "Step 5: Click Start Session button to setup video connection with Azure TTS Talking Avatar service. If everything goes well, you should see a live video with an avatar being shown on the web page.\n",
        "\n",
        "Step 6: Type some text in the Spoken Text text box and click Speak button to send the text to Azure TTS Talking Avatar service. The service will synthesize the text to talking avatar video, and send the video stream back to the browser. The browser will play the video stream. You should see the avatar speaking the text you typed with mouth movement, and hear the voice which is synchronized with the mouth movement.\n",
        "\n",
        "Step 7: You can either continue to type text in the Spoken Text text box and let the avatar speak that text by clicking Speak button, or click Stop Session button to stop the video connection with Azure TTS Talking Avatar service. If you click Stop Session button, you can click Start Session button to start a new video connection with Azure TTS Talking Avatar service."
      ],
      "metadata": {
        "id": "F_hn7gpR-AJ8"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5raKs6t49vkT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}